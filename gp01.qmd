---
title: "Analyzing 311 Service Delivery Across NYC's Income Divide"
subtitle: "STA 9750 Final Project - Mid-Semester Check-In"
author: 
  - "Kyle Shaddox"
  - "Reem Hussein" 
  - "Hyacinthe Sarr"
  - "Juan Jaimes"
  - "Shahria Ahmed"
date: "`r Sys.Date()`"
execute:
  warning: false
  message: false
  echo: true
---

```{r setup, include=FALSE}
# Load required libraries
library(tidyverse)
library(httr2)
library(jsonlite)
library(sf)
library(leaflet)
library(plotly)
library(corrplot)
library(viridis)
library(DT)
library(scales)
library(lubridate)
library(broom)
library(knitr)
library(patchwork)

# Set options
options(scipen = 999)
knitr::opts_chunk$set(
  fig.align = "center",
  cache = TRUE,
  cache.lazy = FALSE
)
```

# Executive Summary

**Research Question:** Does NYC provide faster and more effective 311 service responses to wealthier neighborhoods compared to lower-income communities?

**Key Findings:** [To be completed after analysis]

# 1. Introduction and Motivation

## Problem Statement

The 311 system serves as New York City's primary non-emergency service request platform, handling millions of complaints annually ranging from noise violations to sanitation issues. However, questions persist about whether service delivery is equitable across neighborhoods with different socioeconomic profiles.

## Research Significance

Understanding disparities in 311 service delivery is crucial for:
- Ensuring equitable municipal service provision
- Identifying areas needing improved resource allocation
- Informing policy decisions about urban service delivery
- Promoting environmental and social justice

## Overarching Question

**Does NYC provide faster and more effective 311 service responses to wealthier neighborhoods compared to lower-income communities?**

# 2. Literature Review and Prior Art

## Previous Research

```{r literature-setup}
# Placeholder for literature review synthesis
# Key themes to address:
# - Urban service delivery equity studies
# - 311 system analysis in other cities
# - Socioeconomic factors in municipal services
# - Digital divide and service access patterns
```

**Key Findings from Literature:**
- [To be completed with actual literature review]
- Studies on urban service equity
- Analysis of 311 systems in other major cities
- Research on digital divide impacts on service access

## Research Gap

Our analysis contributes to existing literature by:
- Providing post-pandemic analysis of NYC 311 services
- Examining multiple complaint types across all five boroughs
- Using updated property valuation data for neighborhood wealth assessment
- Implementing predictive modeling for service delivery patterns

# 3. Data Sources and Methodology

## Primary Data Sources

### 3.1 NYC 311 Service Requests (2010-Present)
```{r data-311-info}
# Data source information
cat("Data Source: NYC Open Data\n")
cat("URL: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/\n")
cat("Description: Comprehensive database of all 311 service requests\n")
cat("Key Variables: Location, complaint type, resolution time, status\n")
```

### 3.2 311 Resolution Satisfaction Survey
```{r data-satisfaction-info}
# Data source information  
cat("Data Source: NYC Open Data\n")
cat("URL: https://data.cityofnewyork.us/City-Government/311-Resolution-Satisfaction-Survey/5ijn-vbdv\n")
cat("Description: Customer satisfaction ratings for resolved 311 requests\n")
cat("Key Variables: Resolution satisfaction scores, feedback\n")
```

### 3.3 Property Valuation and Assessment Data
```{r data-property-info}
# Data source information
cat("Data Source: Data.gov\n") 
cat("URL: https://catalog.data.gov/dataset/property-valuation-and-assessment-data-db7c2\n")
cat("Description: Property values and assessments by neighborhood\n")
cat("Key Variables: Property values, assessment data, geographic identifiers\n")
```

## Data Acquisition

### 3.1 Load 311 Service Request Data
```{r}
#| code-fold: true
#| code-summary: "show historical jobs"
#| message: false
#| warning: false


.libPaths("/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RSocrata")
library(RSocrata)
library(dplyr)
library(DT)

suppressPackageStartupMessages({
  library(RSocrata)
  library(dplyr)
})

# Define the file path where you'll save the data
data_file <- "nyc_311_data_2024_jan_apr.rds"

# Check if the file already exists
if (file.exists(data_file)) {
  # If it exists, just read it
  df <- readRDS(data_file)
} else {
  # If it doesn't exist, download it

  # The API endpoint with first 4 months of 2024 filter
  url <- "https://data.cityofnewyork.us/resource/erm2-nwe9.json?$where=created_date>='2024-01-01T00:00:00' AND created_date<'2024-05-01T00:00:00'"

  # Download the data
  df <- read.socrata(url)

  # Save to file
  saveRDS(df, data_file)

}
# Display first 1000 rows only
datatable(head(df, 1000),
          options = list(
            pageLength = 10,
            scrollX = TRUE,
            autoWidth = TRUE
          ),
          filter = 'top',
          class = 'cell-border stripe',
          caption = "Showing first 1,000 rows of data")
```

### 3.2 Load Property Valuation Data
```{r load-property-data}
# Function to load property data
load_property_data <- function() {
  # For development - create sample property data with better structure
  set.seed(42)  # For reproducible results
  sample_property <- tibble(
    neighborhood = paste("Neighborhood", 1:100),
    borough = sample(c("Manhattan", "Brooklyn", "Queens", "Bronx", "Staten Island"), 100, replace = TRUE),
    median_property_value = pmax(100000, rnorm(100, 500000, 200000)),  # Ensure positive values
    median_income = pmax(25000, rnorm(100, 75000, 30000)),             # Ensure positive values
    latitude = runif(100, 40.4, 40.9),
    longitude = runif(100, -74.3, -73.7)
  ) |>
  # Ensure no missing values
  filter(!is.na(median_property_value), !is.na(median_income))
  
  return(sample_property)
}

# Load property data
df_property <- load_property_data()

cat("Property Data Loaded:\n")
cat("Rows:", nrow(df_property), "\n")
cat("Median Property Value Range: $", 
    paste(scales::comma(range(df_property$median_property_value, na.rm = TRUE)), collapse = " - "), "\n")
```

### 3.3 Data Quality Assessment
```{r data-quality}
# Assess data quality for 311 data
assess_data_quality <- function(df, dataset_name) {
  cat("=== Data Quality Assessment:", dataset_name, "===\n")
  
  # Missing values
  missing_summary <- df |>
    summarise(across(everything(), ~sum(is.na(.)))) |>
    pivot_longer(everything(), names_to = "variable", values_to = "missing_count") |>
    mutate(missing_percent = round(missing_count / nrow(df) * 100, 2)) |>
    arrange(desc(missing_percent))
  
  print(missing_summary)
  
  # Data completeness
  cat("\nOverall Completeness:", 
      round((1 - sum(is.na(df)) / (nrow(df) * ncol(df))) * 100, 2), "%\n\n")
}

# Assess both datasets
assess_data_quality(df_311, "311 Service Requests")
assess_data_quality(df_property, "Property Data")
```

## Data Processing and Cleaning

### 3.4 Clean and Process 311 Data
```{r clean-311-data}
# Clean and process 311 data
df_311_clean <- df_311 |>
  # Remove rows with missing essential information
  filter(!is.na(latitude), !is.na(longitude), !is.na(created_date)) |>
  
  # Calculate response time
  mutate(
    response_time_days = as.numeric(difftime(closed_date, created_date, units = "days")),
    year = year(created_date),
    month = month(created_date),
    weekday = wday(created_date, label = TRUE),
    # Categorize complaint types
    complaint_category = case_when(
      str_detect(complaint_type, "Noise") ~ "Noise",
      str_detect(complaint_type, "Parking|Driveway") ~ "Parking",
      str_detect(complaint_type, "Sanitation|Condition") ~ "Sanitation",
      TRUE ~ "Other"
    )
  ) |>
  
  # Filter for reasonable response times (remove outliers)
  filter(response_time_days >= 0 & response_time_days <= 365) |>
  
  # Clean borough names
  mutate(borough = str_to_title(borough))

cat("Cleaned 311 Data:\n")
cat("Rows after cleaning:", nrow(df_311_clean), "\n")
cat("Response time range:", round(range(df_311_clean$response_time_days, na.rm = TRUE), 2), "days\n")
```

### 3.5 Integrate Geographic Data
```{r geographic-integration}
# Create spatial data and integrate with property information
# This would typically involve spatial joins with census tracts or neighborhoods

# For demonstration - assign neighborhoods based on proximity
assign_neighborhoods <- function(df_311, df_property) {
  # Simple nearest neighbor assignment (in practice, use proper spatial joins)
  df_with_neighborhoods <- df_311 |>
    rowwise() |>
    mutate(
      # Find closest neighborhood (simplified calculation)
      neighborhood = {
        distances <- sqrt((latitude - df_property$latitude)^2 + (longitude - df_property$longitude)^2)
        df_property$neighborhood[which.min(distances)]
      }
    ) |>
    ungroup()
  
  # Join with property data - make sure to preserve borough from 311 data
  df_integrated <- df_with_neighborhoods |>
    left_join(df_property |> select(-borough), by = "neighborhood") |>  # Remove borough from property to avoid conflicts
    rename(borough_311 = borough) |>  # Keep original borough from 311 data
    rename(borough = borough_311)     # Use 311 borough as primary
  
  return(df_integrated)
}

# Integrate data
df_integrated <- assign_neighborhoods(df_311_clean, df_property)

cat("Integrated Dataset:\n")
cat("Rows:", nrow(df_integrated), "\n")
cat("Unique Neighborhoods:", length(unique(df_integrated$neighborhood)), "\n")
cat("Columns:", paste(names(df_integrated), collapse = ", "), "\n")
cat("Unique Boroughs:", paste(unique(df_integrated$borough), collapse = ", "), "\n")
```

# 4. Exploratory Data Analysis

## 4.1 Overall 311 Service Patterns
```{r eda-overview}
# Basic statistics
summary_stats <- df_integrated |>
  summarise(
    total_requests = n(),
    avg_response_time = round(mean(response_time_days, na.rm = TRUE), 2),
    median_response_time = round(median(response_time_days, na.rm = TRUE), 2),
    sd_response_time = round(sd(response_time_days, na.rm = TRUE), 2)
  )

kable(summary_stats, caption = "Overall 311 Service Request Statistics")

# Complaint type distribution
complaint_dist <- df_integrated |>
  count(complaint_category, sort = TRUE) |>
  mutate(percentage = round(n / sum(n) * 100, 1))

ggplot(complaint_dist, aes(x = reorder(complaint_category, n), y = n, fill = complaint_category)) +
  geom_col() +
  geom_text(aes(label = paste0(n, " (", percentage, "%)")), hjust = -0.1) +
  coord_flip() +
  labs(
    title = "Distribution of 311 Complaint Types",
    x = "Complaint Category",
    y = "Number of Requests",
    caption = "Data: NYC Open Data"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

# Complaint Types

```{r}
#| code-fold: true
#| code-summary: "show complaints type"
#| message: false
#| warning: false
library(dplyr)
library(ggplot2)
library(scales)
library(stringr)

# Aggregate by complaint_type
complaint_summary <- df %>%
  group_by(complaint_type) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Convert complaint_type to Title Case for readability
complaint_summary <- complaint_summary %>%
  mutate(complaint_type = str_to_title(complaint_type))

# Plot top 15 complaint types with labels
ggplot(complaint_summary %>% slice_max(count, n = 15),
       aes(x = reorder(complaint_type, count), y = count)) +
  geom_col(fill = "#2E86C1", width = 0.7) +
  geom_text(aes(label = comma(count)),
            hjust = -0.05, size = 3.5, family = "sans") +
  coord_flip(clip = "off") +  # allow labels to go past plot area
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  labs(
    title = "Top 15 Complaint Types (Jan–Apr 2024)",
    x = "Complaint Type",
    y = "Number of Complaints"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10)),
    plot.margin = margin(10, 40, 10, 10)  # extra space on right for labels
  )
```
# Complaints by Borough
```{r}
#| code-fold: true
#| code-summary: "show complaints by borough"
#| message: false
#| warning: false


# Aggregate by borough
library(dplyr)
library(ggplot2)
library(scales)
library(stringr)

mako_colors <- c("#0D0887", "#6A00A8", "#B12A90", "#E16462", "#FCA636", "#F0F921")
# Aggregate by borough
borough_summary <- df %>%
  group_by(borough) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  mutate(borough = str_to_title(borough))  # title case

# Plot
ggplot(borough_summary,
       aes(x = count, y = reorder(borough, count), fill = borough)) +
  geom_col(width = 0.6, show.legend = FALSE) +
  geom_text(aes(label = comma(count)),
            hjust = -0.05, size = 4, family = "sans") +  # data labels
  scale_fill_manual(values = mako_colors) +
  scale_x_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +  # fix scientific notation
  labs(
    title = "311 Complaints by Borough (Jan–Apr 2024)",
    x = "Number of Complaints",
    y = "Borough"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10)),
    plot.margin = margin(10, 40, 10, 10)  # extra space for labels
  ) +
  coord_cartesian(clip = "off")  # allow data labels to extend past plot area


```

# Location Types
```{r}
#| code-fold: true
#| code-summary: "show location type"
#| message: false
#| warning: false


# Aggregate by borough
library(dplyr)
library(ggplot2)
library(scales)
library(stringr)

mako_colors <- c("#0D0887", "#6A00A8", "#B12A90", "#E16462", "#FCA636", "#F0F921")
# Aggregate by borough
borough_summary <- df %>%
  group_by(borough) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  mutate(borough = str_to_title(borough))  # title case

# Plot
ggplot(borough_summary,
       aes(x = count, y = reorder(borough, count), fill = borough)) +
  geom_col(width = 0.6, show.legend = FALSE) +
  geom_text(aes(label = comma(count)),
            hjust = -0.05, size = 4, family = "sans") +  # data labels
  scale_fill_manual(values = mako_colors) +
  scale_x_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +  # fix scientific notation
  labs(
    title = "311 Complaints by Borough (Jan–Apr 2024)",
    x = "Number of Complaints",
    y = "Borough"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10)),
    plot.margin = margin(10, 40, 10, 10)  # extra space for labels
  ) +
  coord_cartesian(clip = "off")  # allow data labels to extend past plot area


```

# Average Resolution by Borough

```{r fig.height=6, fig.width=8}
#| code-fold: true
#| code-summary: "show average resolution"
#| message: false
#| warning: false


library(dplyr)
library(ggplot2)
library(lubridate)
library(scales)
library(stringr)

mako_colors <- c("#0D0887", "#6A00A8", "#B12A90", "#E16462", "#FCA636", "#F0F921")
# Ensure date columns are in POSIXct format
df <- df %>%
  mutate(
    created_date = ymd_hms(created_date, tz = "America/New_York"),
    closed_date = ymd_hms(closed_date, tz = "America/New_York")
  )

# Create a new column: time to resolve in days
df <- df %>%
  mutate(
    resolution_time_days = as.numeric(difftime(closed_date, created_date, units = "days"))
  )

# Remove NA or unspecified boroughs
df_clean <- df %>%
  filter(!is.na(borough) & borough != "Unspecified" & !is.na(resolution_time_days))

# Compute average resolution time by borough
borough_resolution <- df_clean %>%
  group_by(borough) %>%
  summarise(avg_resolution_days = mean(resolution_time_days, na.rm = TRUE)) %>%
  mutate(borough = str_to_title(borough))

# Scatter plot
ggplot(borough_resolution, aes(x = reorder(borough, avg_resolution_days), y = avg_resolution_days)) +
  geom_point(size = 4, color = "#E74C3C") +
  geom_text(aes(label = round(avg_resolution_days, 1)),
            vjust = -0.8, size = 4) +
  scale_fill_manual(values = mako_colors) +
  labs(
    title = "Average Resolution Time by Borough (Days)",
    x = "Borough",
    y = "Average Resolution Time (Days)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),  # centered title
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10))
  )

```

## 4.2 Geographic Distribution
```{r eda-geographic}
# Response times by borough
borough_stats <- df_integrated |>
  filter(!is.na(borough)) |>  # Remove any rows with missing borough
  group_by(borough) |>
  summarise(
    request_count = n(),
    avg_response_time = round(mean(response_time_days, na.rm = TRUE), 2),
    median_response_time = round(median(response_time_days, na.rm = TRUE), 2),
    median_property_value = round(mean(median_property_value, na.rm = TRUE), 0),
    .groups = "drop"
  ) |>
  arrange(desc(avg_response_time))

kable(borough_stats, caption = "311 Service Statistics by Borough")

# Visualization - also filter for non-missing borough
p1 <- df_integrated |>
  filter(!is.na(borough)) |>
  ggplot(aes(x = borough, y = response_time_days, fill = borough)) +
  geom_boxplot() +
  labs(
    title = "Response Time Distribution by Borough",
    x = "Borough",
    y = "Response Time (Days)"
  ) +
  theme_minimal() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1))

p2 <- ggplot(borough_stats, aes(x = reorder(borough, avg_response_time), y = avg_response_time, fill = borough)) +
  geom_col() +
  geom_text(aes(label = paste0(avg_response_time, " days")), hjust = -0.1) +
  coord_flip() +
  labs(
    title = "Average Response Time by Borough",
    x = "Borough",
    y = "Average Response Time (Days)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

p1 / p2
```

# 5. Specific Research Questions Analysis

## 5.1 Reem's Analysis: Average Response Time by Neighborhood

**Research Question:** What is the average response time to quality-of-life complaints by neighborhood?

```{r reem-analysis}
# Reem's analysis code here - PLACEHOLDER
cat("=== REEM'S ANALYSIS SECTION ===\n")
cat("Research Question: What is the average response time by neighborhood?\n\n")

# Create placeholder data for demonstration
placeholder_neighborhoods <- tibble(
  neighborhood = paste("Neighborhood", 1:10),
  borough = sample(c("Manhattan", "Brooklyn", "Queens", "Bronx", "Staten Island"), 10, replace = TRUE),
  avg_response_time = round(runif(10, 3, 20), 1),
  request_count = sample(50:200, 10),
  median_income = sample(40000:120000, 10)
)

kable(placeholder_neighborhoods, caption = "PLACEHOLDER: Top 10 Neighborhoods Analysis")

# Placeholder visualization
ggplot(placeholder_neighborhoods, aes(x = median_income, y = avg_response_time, color = borough)) +
  geom_point(aes(size = request_count), alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "PLACEHOLDER: Response Time vs. Neighborhood Income",
    subtitle = "Your team will create the actual analysis",
    x = "Median Income ($)",
    y = "Average Response Time (Days)",
    color = "Borough",
    size = "Request Count"
  ) +
  scale_x_continuous(labels = scales::dollar_format()) +
  theme_minimal()

cat("PLACEHOLDER: Correlation analysis results will go here\n")
```

## 5.2 Sarr's Analysis: Income vs Response Time Relationship

**Research Question:** Is there a relationship between income levels and average 311 response times?

Understanding whether wealthier neighborhoods experience faster service delivery is central to our broader equity question. If response times systematically vary with income, this could reveal structural differences in how municipal resources are allocated.

### Data Preparation

```{r sarr-data, echo=TRUE, message=FALSE, warning=FALSE}

# Sarr's individual cleaned dataset (Jan–Apr 2024)

library(dplyr)
library(lubridate)
library(ggplot2)

# Load the combined 4-month dataset
nyc_311 <- read.csv("nyc311_2024_jan_apr_residential_raw.csv")

# Clean timestamps and compute response times
nyc_311 <- nyc_311 %>%
  mutate(
    created_date   = ymd_hms(created_date, tz = "America/New_York", quiet = TRUE),
    closed_date    = ymd_hms(closed_date,  tz = "America/New_York", quiet = TRUE),
    response_hours = as.numeric(difftime(closed_date, created_date, units = "hours")),
    zip_code       = as.character(incident_zip)
  ) %>%
  filter(
    !is.na(response_hours),
    response_hours >= 0,
    response_hours <= 30 * 24,
    borough %in% c("BRONX","BROOKLYN","MANHATTAN","QUEENS","STATEN ISLAND")
  )

# Aggregate to ZIP level
nyc_zip <- nyc_311 %>%
  group_by(zip_code) %>%
  summarise(
    avg_response    = mean(response_hours, na.rm = TRUE),
    median_response = median(response_hours, na.rm = TRUE),
    n_complaints    = n(),
    .groups = "drop"
  )

# Load ACS income data
nyc_income <- read.csv("nyc_income.csv") %>%
  mutate(zip_code = as.character(zip_code))

# Join datasets
nyc_joined <- nyc_zip %>%
  left_join(nyc_income, by = "zip_code")

glimpse(nyc_joined)

```

Before visualizing the relationship between income and response times, complaint-level data were aggregated to the ZIP Code level and merged with Census median household income. This produces a dataset that allows us to examine whether higher-income neighborhoods tend to receive faster service.

```{r sarr-analysis,echo=TRUE, message=FALSE, warning=FALSE, fig.width=7, fig.height=5}

ggplot(nyc_joined, aes(x = median_income, y = avg_response)) +
  geom_point(alpha = 0.6, color = "#0072B2") +
  geom_smooth(method = "lm", se = TRUE, color = "red", linewidth = 1.1) +
  labs(
    title = "Relationship Between Median Household Income and 311 Response Times",
    subtitle = "ZIP-level comparison across NYC (Jan–Apr 2024)",
    x = "Median Household Income (USD)",
    y = "Average 311 Response Time (Days)"
  ) +
  scale_x_continuous(labels = scales::dollar_format()) +
  theme_minimal()


```

## Interpretation

The scatterplot shows a wide vertical spread of 311 response times across neighborhoods with vastly different income levels. Both **low-income** and **high-income** ZIP Codes experience short and long response times, indicating that income alone does not explain much of the variation.

Several patterns reinforce this:

- **Lower-income ZIP Codes (< \$60,000):** display substantial variability — average response times range from under 40 hours to well above 100 hours.
- **Higher-income ZIP Codes (> \$150,000):** also show variability, though they tend to cluster slightly lower overall.
- **Regression pattern:** the fitted regression line slopes only mildly downward, suggesting that higher income corresponds to marginally shorter response times, but the effect is extremely small.
- **Vertical dispersion:** the vertical spread of points is much larger than any horizontal trend, indicating that operational or geographic factors likely dominate variation in response times.

Overall, this exploratory analysis finds **no strong evidence of income-based inequality** in 311 response times during the January–April 2024 study window. While wealthier ZIP Codes may experience slightly quicker responses on average, the effect is small and overshadowed by wider variation across the city.


## Conclusion

Taken together, these results indicate that **income alone is not a major driver** of 311 response-time differences. The weak correlation, minimal regression effect, and large within-income variability all point to the same conclusion:

Other operational, geographic, or complaint specific factors likely play a much larger role.

To better understand what shapes response-time differences, following sections will examine:

- **Complaint type differences**
- **Complaint volume effects**
- **Geographic variation across neighborhoods**


## 5.3 Juan's Analysis: Complaint Type Resolution by Wealth

**Research Question:** Do certain types of quality-of-life complaints get resolved faster in wealthier neighborhoods?

```{r juan-analysis}
# Juan's analysis code here - PLACEHOLDER
cat("=== JUAN'S ANALYSIS SECTION ===\n")
cat("Research Question: Complaint type resolution by wealth?\n\n")

# Placeholder data for complaint types by wealth
placeholder_complaint_wealth <- tibble(
  complaint_category = rep(c("Noise", "Parking", "Sanitation", "Other"), 2),
  wealth_category = rep(c("Wealthy", "Less Wealthy"), each = 4),
  avg_response_time = c(8.2, 6.5, 7.1, 9.3, 12.1, 10.8, 11.5, 14.2),
  request_count = sample(200:800, 8)
)

kable(placeholder_complaint_wealth, caption = "PLACEHOLDER: Response Times by Complaint Type and Wealth")

# Placeholder visualization
ggplot(placeholder_complaint_wealth, aes(x = complaint_category, y = avg_response_time, fill = wealth_category)) +
  geom_col(position = "dodge") +
  labs(
    title = "PLACEHOLDER: Response Times by Complaint Type and Wealth",
    subtitle = "Your team will analyze actual complaint resolution patterns",
    x = "Complaint Category",
    y = "Average Response Time (Days)",
    fill = "Neighborhood Type"
  ) +
  theme_minimal()

cat("PLACEHOLDER: Statistical comparison tests will go here\n")
```

## 5.4 Kyle's Analysis: Borough and Neighborhood Disparities

**Research Question:** Which boroughs or neighborhoods show the greatest disparities in 311 response times when compared by wealth?

```{r kyle-analysis}
# Kyle's analysis code here - PLACEHOLDER
cat("=== KYLE'S ANALYSIS SECTION ===\n")
cat("Research Question: Borough disparities by wealth?\n\n")

# Placeholder disparity data
placeholder_disparities <- tibble(
  borough = c("Manhattan", "Brooklyn", "Queens", "Bronx", "Staten Island"),
  `High Wealth` = c(6.2, 8.1, 9.3, 11.2, 7.8),
  `Medium Wealth` = c(8.4, 10.2, 11.8, 14.1, 9.9),
  `Low Wealth` = c(12.1, 15.3, 16.8, 19.4, 13.2),
  disparity_high_low = c(-5.9, -7.2, -7.5, -8.2, -5.4)
)

kable(placeholder_disparities, caption = "PLACEHOLDER: Response Time Disparities by Borough and Wealth")

# Placeholder heatmap
disparity_long <- placeholder_disparities |>
  select(borough, disparity_high_low) |>
  mutate(comparison = "High vs Low Wealth")

ggplot(disparity_long, aes(x = comparison, y = borough, fill = disparity_high_low)) +
  geom_tile() +
  geom_text(aes(label = round(disparity_high_low, 1)), color = "white", fontface = "bold") +
  scale_fill_gradient2(
    low = "green", mid = "white", high = "red",
    midpoint = 0, name = "Disparity\n(Days)"
  ) +
  labs(
    title = "PLACEHOLDER: Response Time Disparities Across NYC Boroughs",
    subtitle = "Your team will calculate actual disparities",
    x = "Wealth Comparison",
    y = "Borough"
  ) +
  theme_minimal()

cat("PLACEHOLDER: Geographic disparity analysis and statistical tests will go here\n")
```

## 5.5 Shahria's Analysis: Predictive Modeling

**Research Question:** Can we predict where a complaint came from (neighborhood) based on the complaint characteristics?
### Classification Modeling

To make this project interesting and have real world applications, we decided to add in predictive analytics through machine learning. It started out with a simple question: Can we predict where a complaint came from (neighborhood) based on the complaint?  This can help us determine how long it will take for the complaint to be resolved since we know the average turnaround time for a borough. But this quickly proved difficult and we ran into problems, causing us to pivot three times. 

To predict neighborhood from complaint data, we'll need to build a classification model. Randomforest is the one I’m most familiar with and have used in Python, so I thought it would be fairly straightforward to translate it over into R. For the first test, we pulled data from the 311 complaints dataset, narrowing the timeframe to the first four months of 2024. Just four months ended up having over one million rows. For the initial model, the borough was the target variable and the following columns were the features:

- **complaint_type**
- **descriptor**
- **agency**
- **open_data_channel_type**
- **location_type**
- **hour**
- **day_of_week**
- **month**
- **is_weekend**

How do we measure accuracy? Usually with regression models, you use R-squared which tells you how close the model was to the actual numbers. For classification models, we rely on accuracy or how often we are right (% of correct predictions). There are other metrics to evaluate them but for simplicity’s sake, I went with accuracy. The formula is below:

**Accuracy = (True Positives + True Negatives) / (Total Predictions)**

Using a randomforest classifier model and splitting the dataset into a training and testing set, I ran the model. This however was very disappointing, with a model accuracy of **38.46%**. There were some things we can do to improve the accuracy such as applying a k fold cross validation and grid search to find the optimal parameters for the model. However, this didn’t improve the outcome greatly. Ultimately, I wasn’t confident in saying that we can predict where the complaint came from with a **38%** accuracy. 
```{r}
#| code-fold: true
#| code-summary: "Prepare data for prediction model"
#| message: false
#| warning: false
#| eval: false


library(dplyr)
library(tidyr)
library(lubridate)

# this is the data wrangling part
df_model <- df %>%
  # we remove rows with missing target or key predictors
  filter(!is.na(borough),
         !is.na(complaint_type),
         borough != "Unspecified") %>%

  # take existing created date of complaint and break down hour, day, month. this allows for more granularity
  mutate(
    hour = hour(created_date),
    day_of_week = wday(created_date, label = TRUE),
    month = month(created_date, label = TRUE),
    is_weekend = day_of_week %in% c("Sat", "Sun")
  ) %>%
  #predictive features
  select(
    # target variable
    borough,
    # features we use to predict target variable
    complaint_type,
    descriptor,
    agency,
    open_data_channel_type,
    location_type,
    hour,
    day_of_week,
    month,
    is_weekend
  ) %>%
  # drop missing values
  drop_na()

# print class distribution
table(df_model$borough) %>%
  as.data.frame() %>%
  arrange(desc(Freq)) %>%
  print()
```

```{r}
#| code-fold: true
#| code-summary: "Build Randomforest prediction model"
#| message: false
#| warning: false
#| eval: false


options(repos = c(CRAN = "https://cran.rstudio.com/")) #need CRAN otherwise we get error

#install random forest and other modules as needed 
if (!require("randomForest")) install.packages("randomForest")
if (!require("caret")) install.packages("caret")
if (!require("forcats")) install.packages("forcats")

library(randomForest)
library(caret)
library(dplyr)
library(forcats)  #fct_lump_n()

set.seed(123)

#which columns have too many categories
sapply(df_model, function(x) length(unique(x))) %>%
  sort(decreasing = TRUE) %>%
  print()

#keep only top 20-30 categories for high cardinality features
df_model_prep <- df_model %>%
  mutate(
    #keep only top 30 complaint types, rest become Other
    complaint_type = fct_lump_n(as.factor(complaint_type), n = 30),
    #keep only top 30 descriptors
    descriptor = fct_lump_n(as.factor(descriptor), n = 30),
    #keep only top 20 location types
    location_type = fct_lump_n(as.factor(location_type), n = 20),
    # convert other character columns to factors
    agency = as.factor(agency),
    borough = as.factor(borough),
    day_of_week = as.factor(day_of_week),
    month = as.factor(month)
  )

# split into train/test using standard 80/20 ratio
# use indexing to access and partition rows like in python
train_index <- createDataPartition(df_model_prep$borough, p = 0.8, list = FALSE)
train_data <- df_model_prep[train_index, ]
test_data <- df_model_prep[-train_index, ]

# train Randomforest model
# sometimes model took over 2 hours to run, so limit to n rows to speed it up
if(nrow(train_data) > 50000) {
  train_sample <- train_data %>% sample_n(50000)
} else {
  train_sample <- train_data
}

#assign variable and run model
rf_model <- randomForest(
  borough ~ .,
  data = train_sample,
  ntree = 50, #100 ntrees was taking too long so I used 50
  importance = TRUE,
  na.action = na.omit
)
#make predictions
predictions <- predict(rf_model, test_data)
# confusion matrix for evaluating model
confusion_matrix <- confusionMatrix(predictions, as.factor(test_data$borough))
```


What if we expand the timeframe? We can take _100,000 rows_ from each month and get a years worth of data for the model. The complaints from the first four months were primarily based on heat and hot water, which made sense given that it was during winter. But if we expanded to the rest of the year, complaints would balance out across the seasons. It would also give the model better quality data to predict on. So I reran the datapull, limiting to _100,000 rows_ for each months and reran the model. This resulted in a model accuracy of **39%** which was barely an improvement. 

```{r}
#| code-fold: true
#| code-summary: "Download stratified sample of 2024 data"
#| message: false
#| warning: false
#| eval: false

#r socrata gives issues, so have to map out the path where it is downloaded so R can find it 
.libPaths("/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RSocrata")
library(RSocrata)
library(dplyr)

# i get alot of messages showing download progress. dont need it, so suppress it for now
suppressPackageStartupMessages({
  library(RSocrata)
  library(dplyr)
})

# this is the name of the file that will be saved
data_file <- "nyc_311_data_2024_stratified.rds"
#check if the file already exists
if (file.exists(data_file)) {
  df_2024 <- readRDS(data_file) #if it exists, just read it. no need to redownload again
} else {
  # create empty list to store monthly dataframes and concat at the end
  monthly_data_list <- list()
  # for loop through each month of 2024
  for(month in 1:12) {
    #  reate date strings for the month
    start_date <- sprintf("2024-%02d-01T00:00:00", month)
    # calculate end date
    if(month == 12) {
      end_date <- "2025-01-01T00:00:00"
    } else {
      end_date <- sprintf("2024-%02d-01T00:00:00", month + 1)
    }
    # download data for each month using the start and end date, which store each months date in them.     # set limit to 100000 rows. this takes the first 100000 rows, its not fully random
    url <- paste0(
      "https://data.cityofnewyork.us/resource/erm2-nwe9.json?",
      "$where=created_date>='", start_date, "' AND created_date<'", end_date, "'",
      "&$limit=100000"  
    )
    #call socrata package
    month_data <- read.socrata(url)
    #store in list
    monthly_data_list[[month]] <- month_data
  }
  #concact all lists into a dataframe 
  df_2024 <- bind_rows(monthly_data_list)
  #save to file
  saveRDS(df_2024, data_file)
}
#print out stats
df_2024 %>%
  mutate(month = format(as.Date(created_date), "%Y-%m")) %>%
  count(month) %>%
  arrange(month) %>%
  print()
```

```{r}
#| code-fold: true
#| code-summary: "Prepare 2024 data for borough prediction"
#| message: false
#| warning: false
#| eval: false

library(dplyr)
library(lubridate)
library(tidyr)

#create features for borough prediction
df_classification <- df_2024 %>%
  # remove rows with missing borough
  filter(!is.na(borough), borough != "Unspecified") %>%
# take existing created date of complaint and break down hour, day, month. this allows for more granularity
  mutate(
    hour = hour(created_date),
    day_of_week = wday(created_date, label = TRUE),
    month = month(created_date, label = TRUE),
    is_weekend = day_of_week %in% c("Sat", "Sun")
  ) %>%
  
  # select relevant features
  select(
    borough,   # target variable
    complaint_type,   # features we use to predict target variable
    descriptor,
    agency,
    open_data_channel_type,
    location_type,
    hour,
    day_of_week,
    month,
    is_weekend
  ) %>%
  # drop na
  drop_na(complaint_type, agency)
#print stats
table(df_classification$borough) %>%
  as.data.frame() %>%
  arrange(desc(Freq)) %>%
  print()
```


```{r}
#| code-fold: true
#| code-summary: "Build Randomforest classification model on 2024 data"
#| message: false
#| warning: false
#| eval: false

options(repos = c(CRAN = "https://cran.rstudio.com/")) #need CRAN link or you get an error

#install libraries if needed
if (!require("randomForest")) install.packages("randomForest")
if (!require("caret")) install.packages("caret")
if (!require("forcats")) install.packages("forcats")

library(randomForest)
library(caret)
library(dplyr)
library(forcats)

#seed for reproducibility 
set.seed(123)

# final modeling datasetand reduce number of values to 20-30
df_model <- df_classification %>%
  mutate(
    complaint_type = fct_lump_n(as.factor(complaint_type), n = 30),
    descriptor = fct_lump_n(as.factor(descriptor), n = 30),
    location_type = fct_lump_n(as.factor(location_type), n = 20),
    agency = as.factor(agency),
    borough = as.factor(borough),
    day_of_week = as.factor(day_of_week),
    month = as.factor(month)
  ) %>%
  drop_na()

# split into train/test using standard 80/20 ratio
# use indexing to access and partition rows like in python
train_index <- createDataPartition(df_model$borough, p = 0.8, list = FALSE)
train_data <- df_model[train_index, ]
test_data <- df_model[-train_index, ]
# train Randomforest model
# sometimes model took over 2 hours to run, so limit to n rows to speed it up
if(nrow(train_data) > 100000) {
  train_sample <- train_data %>% sample_n(100000)
} else {
  train_sample <- train_data
}

# train Randomforest model
rf_model <- randomForest(
  borough ~ .,
  data = train_sample,
  ntree = 50, #100 ntrees took way too long, limit to 50
  importance = TRUE,
  na.action = na.omit
)

#make predictions
predictions <- predict(rf_model, test_data)
#evaluate model with confusion matrix
confusion_matrix <- confusionMatrix(predictions, as.factor(test_data$borough))
cat("Overall Accuracy:", round(confusion_matrix$overall['Accuracy'] * 100, 2), "%\n\n")
```



I ended up pivoting a third time and using the 311 resolution satisfaction dataset instead. This was much smaller, **~436,000 rows**, and had a much longer time frame rather than the first four months. Having the resolution satisfaction data was useful as we can see satisfaction across the boroughs. But right away there was an issue: the satisfaction metric in this dataset was qualitative. It ranged from strongly agree to strongly disagree. Using conditional logic, we can assign a numerical value to each and then run the model.

Running the Randomforest model through this yielded a model accuracy of **45.26%**.  Even with cross validation and gridsearch, we could not get a higher accuracy value. So for this project, we decided to keep it as is. 45% of the time, our model will accurately tell you which borough a complaint came from.

# Exploratory Data Analysis
```{r}
#| code-fold: true
#| code-summary: "Download 311 Satisfaction Survey Data"
#| message: false
#| warning: false

#r socrata gives issues, so have to map out the path where it is downloaded so R can find it 
.libPaths("/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RSocrata")
library(RSocrata)
library(dplyr)
library(DT)

#suppress download messages
suppressPackageStartupMessages({
  library(RSocrata)
  library(dplyr)
})

# this is the file name of downloaded 
data_file <- "nyc_311_satisfaction_survey.rds"

#check if the file already exists
if (file.exists(data_file)) {
  #if it does, just read it. no need to redownload
  df_satisfaction <- readRDS(data_file)
} else {
  #if it doesn't exist, download it
  # API URL endpoint for satisfaction survey data
  url <- "https://data.cityofnewyork.us/resource/5ijn-vbdv.json"
  #download the data
  df_satisfaction <- read.socrata(url)
  # save to file
  saveRDS(df_satisfaction, data_file)
}

# show first 1000 rows in nice clean datatable
datatable(head(df_satisfaction, 1000),
          options = list(
            pageLength = 5,
            scrollX = TRUE,
            autoWidth = TRUE
          ),
          filter = 'top',
          class = 'cell-border stripe',
          caption = "Showing first 1,000 rows of 311 Satisfaction Survey data")
```

Before we run the model, we convert the categorical satisfaction metrics to numerical. Let's look at the total count of each satisfaction option.
```{r}
#| code-fold: true
#| code-summary: "Data wrangling"
#| message: false
#| warning: false
library(dplyr)
library(DT)

# create function to convert satisfaction text to numeric score
convert_satisfaction_to_numeric <- function(satisfaction_text) {
  case_when(
    grepl("Strongly Agree|Very Satisfied|Strongly Satisfied", satisfaction_text, ignore.case = TRUE) ~ 5,
    grepl("^Agree|^Satisfied", satisfaction_text, ignore.case = TRUE) ~ 4,
    grepl("Neutral|Neither", satisfaction_text, ignore.case = TRUE) ~ 3,
    grepl("Dissatisfied|^Disagree", satisfaction_text, ignore.case = TRUE) ~ 2,
    grepl("Strongly Disagree|Strongly Dissatisfied|Very Dissatisfied", satisfaction_text, ignore.case = TRUE) ~ 1,
    TRUE ~ NA_real_
  )
}
# Create table of unique values in overall_satisfaction
satisfaction_table <- as.data.frame(table(df_satisfaction$overall_satisfaction, useNA = "ifany"))
colnames(satisfaction_table) <- c("Overall Satisfaction", "Unique Count")
```

```{r}
#show data in clean formatted datatable
datatable(satisfaction_table, 
          options = list(pageLength = 10, dom = 't'),
          rownames = FALSE) %>% formatCurrency('Unique Count', currency = "", interval = 3, mark = ",", digits = 0)
```

Once we remove missing values and clean up the data, we have a final dataframe that we can pass into the randomforest model
```{r}
#| code-fold: true
#| code-summary: "Data wrangling"
#| message: false
#| warning: false
#| 
#prepare modeling dataset
df_model_satisfaction <- df_satisfaction %>%
  #filter for rows with borough information
  filter(!is.na(borough), borough != "Unspecified", borough != "") %>%
  filter(!is.na(overall_satisfaction)) %>%
  # convert satisfaction to numeric
  mutate(
    satisfaction_score = convert_satisfaction_to_numeric(overall_satisfaction),
    #create numeric versions of year and month
    year = as.numeric(year),
    month = as.numeric(month)
  ) %>%
  # remove na
  filter(!is.na(satisfaction_score))

# Create summary table
summary_df <- df_model_satisfaction %>% 
  select(borough, satisfaction_score, year, month) %>%
  summary() %>%
  as.data.frame() %>%
  tidyr::separate(Freq, into = c("Statistic", "Value"), sep = ":", extra = "merge") %>%
  select(Variable = Var2, Statistic, Value) %>%
  filter(!is.na(Value))
```


```{r}
#| code-fold: true
#| code-summary: "Build classification model"
#| message: false
#| warning: false
library(tidyr)
library(DT)
options(repos = c(CRAN = "https://cran.rstudio.com/")) #need CRAN link or you get error
 
##install packages if needed
if (!require("randomForest")) install.packages("randomForest")
if (!require("caret")) install.packages("caret")
if (!require("forcats")) install.packages("forcats")
library(randomForest)
library(caret)
library(dplyr)
library(forcats)

#random number seed for reproducibility
set.seed(123)

#prepare final modeling dataset
df_final <- df_model_satisfaction %>%
  mutate(
    #convert categorical variables to factors and reduce values to 50. randomforest is limited to 53 features
    complaint_type = fct_lump_n(as.factor(complaint_type), n = 50),
    descriptor = fct_lump_n(as.factor(descriptor), n = 50),
    agency = as.factor(agency),
    dissatisfaction_reason = fct_lump_n(as.factor(dissatisfaction_reason), n = 50),
    borough = as.factor(borough)
  ) %>%
  select(borough, satisfaction_score, complaint_type, descriptor, agency,
         dissatisfaction_reason, year, month) %>%
  drop_na()

# Borough distribution table
borough_table <- as.data.frame(table(df_final$borough))
colnames(borough_table) <- c("Borough", "Count")
```

Let's take a look at complaint distribution by borough. Brooklyn has the most complaints out of any borough! Nearly double the complaints of Manhattan!
```{r}
#| code-fold: true
#| code-summary: "Borough distribution"
#| message: false
#| warning: false
library(ggplot2)

#plot compplaints distribution by borough
ggplot(borough_table %>% arrange(desc(Count)), 
       aes(x = reorder(Borough, Count), y = Count)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Complaint Distribution by Borough", #adjust title x and y values as needed
       x = "Borough",
       y = "Count of Complaints") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```


```{r}
#| code-fold: true
#| code-summary: "Build classification model"
#| message: false
#| warning: false
#| 

# Split into 80/20 standard training and test set. use indexing to access df rows 
train_index <- createDataPartition(df_final$borough, p = 0.8, list = FALSE)
train_data <- df_final[train_index, ]
test_data <- df_final[-train_index, ]

#train Randomforest model
rf_model <- randomForest(
  borough ~ .,
  data = train_data,
  ntree = 100, #100 ntrees depth
  importance = TRUE,
  na.action = na.omit
)

#make predictions
predictions <- predict(rf_model, test_data)
#evaluate model
confusion_matrix <- confusionMatrix(predictions, test_data$borough)
```

At last, we ran the model! How accurate is it? Not the best, but still better than our initial model 
```{r}
#| code-fold: true
#| code-summary: "Model accuracy"
#| message: false
#| warning: false
#| 
# create model results summary table to show accuracy
results_summary <- data.frame(
  Metric = c("Model Accuracy", "Baseline (random guessing)", "Number of Boroughs"),
  Value = c(
    paste0(round(confusion_matrix$overall['Accuracy'] * 100, 2), "%"),
    paste0(round(100 / length(unique(df_final$borough)), 2), "%"),
    length(unique(df_final$borough))
  )
)

#show in datable
datatable(results_summary, 
          options = list(pageLength = 10, dom = 't'),
          rownames = FALSE)
```


Let's take a look at the confusion matrix below. It's a big confusing, but the simplest way to interpret this is that it shows where the model got it right vs. where it got confused. Diagonal values are correct values and anything outside are incorrect

**Cell (Brooklyn, Manhattan) = 2,565** means the model predicted Brooklyn 2,565 times when it was actually Manhattan \
**Cell (Queens, Queens) = 3,961** means the model correctly predicted Queens 3,961 times 
```{r}
#| code-fold: true
#| code-summary: "Model evaluation"
#| message: false
#| warning: false

# show confusion matrix as datatable
confusion_df <- as.data.frame.matrix(confusion_matrix$table)
confusion_df <- cbind(Predicted = rownames(confusion_df), confusion_df)

#get the names of all numeric columns
numeric_cols <- colnames(confusion_df)[colnames(confusion_df) != "Predicted"]

#format datatable numbers in clean presentable way
datatable(confusion_df, 
          options = list(pageLength = 10, dom = 't'),
          rownames = FALSE) %>% 
  formatCurrency(numeric_cols, currency = "", interval = 3, mark = ",", digits = 0)
```


Which features were most important in helping the model predict which borough the complaint is from? Description followed by complaint type. It appears that certain boroughs have specific complaints. For this graph, we use a metric called MeanDecreaseGini which assesses how much a feature reduces uncertainty when making predictions.
```{r}
#| code-fold: true
#| code-summary: "Visualize feature importance"
#| message: false
#| warning: false

library(ggplot2)

#put most important features in a nice datatable
importance_df <- importance(rf_model) %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Feature") %>%
  arrange(desc(MeanDecreaseGini))

#plot most important features in a nice formatted bargraph
ggplot(importance_df,
       aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_col(fill = "#0D0887") +
  coord_flip() +
  labs(#edit title subttile x and y axis names
    title = "Which Features Best Predict Borough from Satisfaction Data?",
    subtitle = "Higher values = more important for prediction",
    x = "Feature",
    y = "Importance Score"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(size = 10)
  )
```

Let's look at the number one complaint from each borough. Illegal parking is the biggest complaint type in Brooklyn and Queens. Imagine you are a homeowner and some random car is parked in your driveway. That's usually the case for a lot of residential areas and it drives people crazy. For Manhattan, we notice a lower number because there are fewer areas to park in a crowded borough.

```{r}
#| code-fold: true
#| code-summary: "Top complaints by borough"
#| message: false
#| warning: false
library(dplyr)
library(ggplot2)

#get the top 3 complaint types for each borough
top_complaints_by_borough <- df_final %>%
  group_by(borough, complaint_type) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(borough) %>%
  slice_max(order_by = count, n = 3) %>% #show top 3 by complaint count
  ungroup() %>%
  arrange(borough, desc(count)) %>%
  mutate(complaint_type = factor(complaint_type))

#create grouped bar chart
ggplot(top_complaints_by_borough, 
       aes(x = reorder(borough, -count), y = count, fill = complaint_type)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Top 3 Complaint Types by Borough", #adjust title x and y values
       x = "Borough",
       y = "Number of Complaints",
       fill = "Complaint Type") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
        legend.position = "bottom",
        axis.text.x = element_text(angle = 0))
```

As a last test of the Randomforest model, we'll take a random complaint and fill in all the necessary metrics. We already know which borough this complaint is from. Our goal here is to see if the model is able to correctly predict the borough.

We have a section in the code chunk below where we can enter values for each complaint metric. The output below is a sample complaint:
```{r}
#| code-fold: true
#| code-summary: "Borough Prediction"
#| message: false
#| warning: false
library(dplyr)
library(DT)

# create function to predict borough based on input features
predict_borough <- function(satisfaction_score, complaint_type, descriptor, 
                           agency, dissatisfaction_reason, year, month) {
  
  # create input dataframe with proper factor levels from training data. we need a dataframe input otherwise model will return error
  input_data <- data.frame( #these are the columns 
    satisfaction_score = satisfaction_score, 
    complaint_type = factor(complaint_type, levels = levels(train_data$complaint_type)),
    descriptor = factor(descriptor, levels = levels(train_data$descriptor)),
    agency = factor(agency, levels = levels(train_data$agency)),
    dissatisfaction_reason = factor(dissatisfaction_reason, levels = levels(train_data$dissatisfaction_reason)),
    year = year,
    month = month
  )
  
  # Make prediction
  prediction <- predict(rf_model, input_data, type = "response")
  probabilities <- predict(rf_model, input_data, type = "prob")
  
  # Create results dataframe -
  results <- data.frame(
    Borough = colnames(probabilities), #borough
    Probability = as.numeric(probabilities[1,]) #respective probability of getting borough
  ) %>%
    arrange(desc(Probability)) %>%
    mutate(Probability = paste0(round(Probability * 100, 2), "%"))
  
  return(list(
    predicted_borough = as.character(prediction),
    probabilities = results
  ))
}


#Inputs: adjust these with manual inputs to replicate complaint
user_satisfaction_score <- 1
user_complaint_type <- "Blocked Driveway"
user_descriptor <- "No Access"
user_agency <- "NYPD"
user_dissatisfaction_reason <- "The Agency did not correct the issue."
user_year <- 2024
user_month <- 1

# make prediction based on every feature used
example_prediction <- predict_borough(
  satisfaction_score = user_satisfaction_score,
  complaint_type = user_complaint_type,
  descriptor = user_descriptor,
  agency = user_agency,
  dissatisfaction_reason = user_dissatisfaction_reason,
  year = user_year,
  month = user_month
)

#display sample complaint features in clean table
input_summary <- data.frame(
  Feature = c("Satisfaction Score", "Complaint Type", "Descriptor", 
              "Agency", "Dissatisfaction Reason", "Year", "Month"),
  Value = c(user_satisfaction_score, user_complaint_type, user_descriptor,
            user_agency, user_dissatisfaction_reason, user_year, user_month)
)

datatable(input_summary, 
          options = list(pageLength = 10, dom = 't'),
          rownames = FALSE)
```

```{r}
#| code-fold: true
#| code-summary: "Filter specific complaint"
#| message: false
#| warning: false
#| eval: false
library(dplyr)
library(DT)

# filter for the exact complaint we want to use to test. we can plug this into the model to validate
specific_complaint <- df_final %>%
  filter(
    borough == "BROOKLYN",
    satisfaction_score == 1,
    complaint_type == "Blocked Driveway",
    descriptor == "No Access",
    agency == "NYPD",
    dissatisfaction_reason == "The Agency did not correct the issue.",
    year == 2024,
    month == 1
  )


#show similar complaints from same borough
similar_complaints <- df_final %>%
  filter(
    borough == "BROOKLYN",
    complaint_type == "Blocked Driveway"
  ) %>%
  head(10)

#format output in clean datatable
datatable(specific_complaint, 
          options = list(pageLength = 5, scrollX = TRUE),
          rownames = FALSE)
```


This is a real complaint from Brooklyn. We know that average turnaround time is roughly **12 days** for a complaint from this borough. Where does our model predict the complaint came from? There's roughly a 45% that we will be accurate here. 
```{r}
cat("Predicted Borough:", example_prediction$predicted_borough, "\n\n")
```

Below is the model's probability of outputting a borough based on the complaint. 
```{r}
datatable(example_prediction$probabilities, 
          options = list(pageLength = 10, dom = 't'),
          rownames = FALSE)
```

# 6. Statistical Analysis and Validation

## 6.1 Correlation Analysis
```{r correlation-analysis}
# Correlation matrix of key variables - PLACEHOLDER
# Your team will replace this with actual correlation analysis

# Create sample correlation data for demonstration
set.seed(123)
sample_correlations <- matrix(
  c(1.0, -0.3, -0.4,
    -0.3, 1.0, 0.8,
    -0.4, 0.8, 1.0),
  nrow = 3,
  dimnames = list(
    c("Response Time", "Median Income", "Property Value"),
    c("Response Time", "Median Income", "Property Value")
  )
)

# Visualize correlation matrix
corrplot(sample_correlations, method = "color", type = "upper", 
         addCoef.col = "black", tl.col = "black", tl.srt = 45,
         title = "Correlation Matrix: Response Time vs Wealth Indicators")

cat("PLACEHOLDER: Your team will calculate actual correlations here\n")
cat("Sample correlation (Response Time vs Income):", sample_correlations[1,2], "\n")
```

## 6.2 Regression Analysis
```{r regression-analysis}
# Multiple regression model - PLACEHOLDER
# Your team will implement the actual regression analysis

cat("PLACEHOLDER: Multiple Regression Analysis\n")
cat("===========================================\n")
cat("Model: response_time ~ median_income + property_value + complaint_type + borough\n\n")

# Create placeholder regression results table
placeholder_results <- tibble(
  term = c("(Intercept)", "median_income", "median_property_value", 
           "complaint_categoryNoise", "complaint_categorySanitation", 
           "boroughBrooklyn", "boroughQueens"),
  estimate = c(15.2, -0.00012, -0.000008, 2.1, 1.8, -0.5, 0.3),
  std.error = c(1.2, 0.00003, 0.000002, 0.8, 0.9, 0.6, 0.7),
  statistic = c(12.7, -4.0, -4.0, 2.6, 2.0, -0.8, 0.4),
  p.value = c(0.000, 0.000, 0.000, 0.009, 0.045, 0.424, 0.689),
  p_value_formatted = c("< 0.001", "< 0.001", "< 0.001", "< 0.01", "< 0.05", "0.424", "0.689")
)

kable(placeholder_results, caption = "PLACEHOLDER: Multiple Regression Results")

cat("\nR-squared: 0.XXX (to be calculated)\n")
cat("Adjusted R-squared: 0.XXX (to be calculated)\n")
cat("F-statistic: XXX on X and XXX DF, p-value: < 0.001\n")
```

# 7. Key Findings and Insights

## 7.1 Summary of Results

```{r summary-results}
# Create summary table of key findings
key_findings <- tibble(
  Research_Question = c(
    "Average response time by neighborhood (Reem)",
    "Income vs response time relationship (Hyacinthe)", 
    "Complaint type resolution by wealth (Juan)",
    "Borough disparities by wealth (Kyle)",
    "Predictive modeling (Shahria)"
  ),
  Key_Finding = c(
    ifelse(exists("cor_test") && !is.null(cor_test$estimate), 
           paste("Correlation coefficient:", round(cor_test$estimate, 3)),
           "Correlation analysis pending"),
    "ANOVA analysis of income quintiles",
    "Response time differences by complaint type and wealth",
    "Geographic disparity analysis",
    "Classification model for neighborhood prediction"
  ),
  Statistical_Significance = c(
    ifelse(exists("cor_test") && !is.null(cor_test$p.value) && cor_test$p.value < 0.05, 
           "Significant", "Analysis pending"),
    "To be determined",
    "Multiple comparisons needed",
    "Descriptive analysis",
    "Classification accuracy assessment"
  )
)

kable(key_findings, caption = "Summary of Key Research Findings")
```

## 7.2 Policy Implications

**Immediate Recommendations:**
- [To be completed based on actual findings]
- Focus resources on neighborhoods with longest response times
- Implement performance monitoring by wealth quintile
- Address complaint type disparities

**Long-term Strategies:**
- Develop equity-based service delivery metrics
- Improve resource allocation algorithms
- Enhanced community engagement in underserved areas

# 8. Limitations and Future Work

## 8.1 Study Limitations

- **Temporal Scope:** Analysis focused on recent data only
- **Geographic Resolution:** Neighborhood assignments may lack precision
- **Confounding Variables:** Other factors affecting response times not captured
- **Data Quality:** Potential reporting biases in 311 system

## 8.2 Future Research Directions

- **Causal Analysis:** Implement quasi-experimental designs
- **Temporal Analysis:** Examine trends over longer time periods
- **Service Quality:** Incorporate satisfaction and resolution effectiveness
- **Community Factors:** Include additional sociodemographic variables

# 9. Conclusion

[To be completed after final analysis]

The analysis of NYC's 311 service delivery system reveals [key findings]. These results have important implications for urban service equity and suggest areas for policy intervention to ensure fair service delivery across all neighborhoods.

# 10. References

1. [Literature citations to be added]
2. NYC Open Data Portal. 311 Service Requests from 2010 to Present. 
3. NYC Open Data Portal. 311 Resolution Satisfaction Survey.
4. Data.gov. Property Valuation and Assessment Data.

# Appendix

## A.1 Additional Visualizations

```{r additional-viz, eval=FALSE}
# Space for additional exploratory visualizations
# Interactive maps, detailed breakdowns, etc.
```

## A.2 Detailed Statistical Output

```{r detailed-stats, eval=FALSE}
# Detailed statistical test results
# Additional model diagnostics
# Sensitivity analyses
```

## A.3 Data Processing Code

```{r data-processing-detail, eval=FALSE}
# Detailed data cleaning steps
# API interaction code
# Geographic processing functions
```

---

*This document was generated using Quarto. For questions about this analysis, please contact the project team.*
